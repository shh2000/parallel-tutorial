Include simple case, not appliable models. E.g. MLP, matmul, a single transformer self-attention layer
